\chapter{Implementation} 

\section{GraphAPI}

The GraphAPI can be queried with a single POST request.
Here's the code to compute the batch request:

\begin{verbatim}
[{'method': 'GET',
    'relative_url': 'search?q={0}
    &type=page&locale=it_IT&limit=50
    &fields=id,name,link,about,email,
    location,description,category,
    contact_address,general_info,
    general_manager,mission,phone,
    website,members,cover,likes'
    .format(
        quote(unicode.encode(company_name,
        encoding='utf-8')))
} for company_name in company_list
] + [
{'method': 'GET',
    'relative_url':'search?q={0}
    &type=user&locale=it_IT&limit=50
    &fields=id,name,link,about,email,
    bio,work,location,description,
    address,hometown,website,cover'
    .format(
        quote(unicode.encode(company_name,
        encoding='utf-8')))
} for company_name in company_list
]
        
\end{verbatim}

\section{Dandelion API}

Dandelion API is a Semantic Text Analytics as a service product developed by Spaziodati.
Dandelion is a product that implement context intelligence: it extracts meaning from an unstructured text and put it in context, obtaining actionable data.
Instead of classic NLP technologies, Dandelion API leverages its underlying Knowledge Graph, without relying on traditional NLP pipelines. This makes it faster, more scalable, easier to customize and natively language independent. Dandelion API works well even on short and malformed texts in English, French, German, Italian and Portuguese.
The main APIs available on Dandelion are: 
\begin{itemize}
    \item ENTITY EXTRACTION: find mentions of places, persons, brands and events in documents and social media. Easily get additional data about the entities.
    \item TEXT \& CONTENT CLASSIFICATION: classify multilingual text into standard, pre-defined taxonomies or build your own custom classification scheme in minutes.
    \item SENTIMENT ANALYSIS: identify whether the expressed opinion in short texts (like product reviews) is positive, negative, or neutral.
    \item KEYWORDS / CONCEPTS EXTRACTION: automatically identify important, contextually relevant, concepts and key-phrases in articles and social media posts.
    \item SEMANTIC SIMILARITY: compare two texts and compute their syntactic and semantic similarity. Understand when two texts are about the same subject.
    \item ARTICLE EXTRACTION: extract clean text article from newspapers, blogs and other websites. Remove boilerplate and advertising and get the article full text and images.
\end{itemize}

\section{Atoka}

Atoka is a lead generation business-to-business tool that helps companies to find new customers, receive news and updates from them and to generate prospect lists with few clicks.
Thanks to the partership with Cerved Group, an Italian rating agency, and the usage of semantic analysis algorithms, Atoka offers affordable information about more than 6 millions Italian companies and economic subjects.
Atoka provide addresses, contacts, websites, social networks links, management  team composition, corporate structure, economic and financial data.
Atoka also enables new search options that overtake tradictional filters (like ATECO code), enabling the customer to make more precise and articulated queries.

\section{Scikit-learn}

Scikit-learn (formerly scikits.learn) is an open source machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.

\section{Azkaban Flow}
The automation of the entire project was implemented with Azkaban.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{img/azk_base.png}
\caption{Interface of Azkaban}
\label{}
\end{figure}

Azkaban is a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\cite{azka}
It presents many features:
\begin{itemize}
    \item Compatible with any version of Hadoop
    \item Easy to use web UI
    \item Simple web and http workflow uploads
    \item Project workspaces
    \item Scheduling of workflows
    \item Modular and pluginable
    \item Authentication and Authorization
    \item Tracking of user actions
    \item Email alerts on failure and successes
    \item SLA alerting and auto killing
    \item Retrying of failed jobs
\end{itemize}
Azkaban was designed primarily with usability in mind. It has been running at LinkedIn for several years, and drives many of their Hadoop and data warehouse processes.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{img/azk_flow.jpg}
\caption{Spaziodati's Facebook Azkaban Flow}
\label{}
\end{figure}

At Spaziodati Azkaban is scheduled once a month and the results are loaded on an Amazon S3 machine. The flow starts reading the Atoka Index and getting the anagraphic data used for computing the features and creating a new elastic search index in order not to modify or kill the production one (that keeps Atoka online). For finding information and normalizing data the flow was developed also to create a new instance of DataTXT, as before, not to touch in any ways the production one. 



