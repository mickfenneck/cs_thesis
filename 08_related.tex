\chapter{Related Work} 

%\section{Related Work}
% A description of other related approaches you have seen in the literature
% TODO(willi,luca) trovare lavori correlati 
When the project started the first thing that has been taken in consideration was a similar project previously developed at Spaziodati. The aim of that project was to match website crawled from Italian net with the same company index that was used for this project.

The first step in order to get the official website classification was building a crawler for Italian websites network that could populate an index. That bot crawled all the websites looking for the same informations found in the company index (descriptions, emails, entities, location, social media and obviously the website URL).

After having created and populated the index, information were cleaned and normalized with Dandelion, in a really similar way of what was done in the Facebook project. Once the data was made comparable, as done here, a proprietary algorithm generated a similarity score for each of the websites' fields.

One time the scores were given, the team that was working on the project faced our same problem of choosing the best algorithm for the classification. Unfortunately, instead of having a similar project as we had, they had nothing to compare the project with and to choose the algorithm on the high level so they tried different solutions and took the best one.

Many algorithms were tried but the main two were:
\begin{itemize}
\item Random Forest
\item Support Vector Machine (SVM).
\end{itemize}

After the tests, the team decided to choose Random Forest mainly for performance of precision and recall over that dataset (obviously this information can't be generalized all over the data available in the world).

In that case Random Forest was the more convenient because it returned values of features that were really useful in order to evaluate the data gathered and to decide how deep the crawler needed to look in the process of data extraction fro the web.
A second important fact that drove the team in the algorithm decision was that the training time was far less than any other method, a fundamental consideration taking care of the fact that the index need to be continuously updated. Furthermore, Random Forest gives the developer the possibility to work with categorical features without too many manipulations. 
Last but not least, Random Forest in the best algorithm that an analyst can choose in order to avoid the overfitting problem.

As revealed before, when we had to choose the best algorithm to classify Facebook pages data, instead of taking care of testing the different algorithms - a process that really makes a company waste a lot of time and money -  we decided to compare the data available in the facebook index with the data available in the website's. The result was that the informations were really similar - as said before the two indexes were cleaned and normalized with really alike methods - so we directly decided to use Random Forest for our classification. Before taking the final decision we also decided to try SVM too because we wanted to be 100\% sure about the fact that it was the best also in our case but in less that a week we faced that, apart from the fact that was really harder to configure it right, it was really too slow for our needs and we firmly opted for Random Forest.



