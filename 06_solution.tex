\chapter{Solution} 

The following paragraph contains a description of how the solution to the problem was developed and why certain choices were made.

\section{Getting Possible Facebook Pages}

The first module of the project has the task to get company names contained in the Atoka index and query Facebook Graph via Facebook GraphAPI in order to get a list of possible compatible pages.

\begin{comment}
\begin{figure}
\centering
\begin{tikzpicture}
 [
    >=stealth,
    node distance=3.1cm,
    database/.style={
      cylinder,
      shape border rotate=90,
      aspect=0.25,
      draw
    }
  ]
    \node[database] (db1) at (0,0) {Atoka Index};
    \node[database,right of=db1] (db2) {Facebook Graph};
    \node[database,right of=db2] (db3) {Atoka Facebook};
    
    \draw[->,blue!50] (db1) -- ++(0,1.5) -- ($(db2)+(0,1.5)$) node[black,midway,above,font=\scriptsize]{Graph API POST Request}  -- (db2) ;
    \draw[->,blue!50] (db2) -- ++(0,-1) -- ($(db3)-(0,1)$) node[black, midway,below,font=\scriptsize]{Graph API JSON Response} -- (db3) ;
  \end{tikzpicture}
    \caption{Process of data acquisition from Facebook GraphAPI}
    \label{Fig.2}
\end{figure}
\end{comment}


\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{img/atoka_fb.png}
\caption{Process of data acquisition from Facebook GraphAPI.}
\label{Fig.3}
\end{figure}

For each company contained in Atoka index, the module generate a list of different possible alternatives to the legal name. This approach extends the time spent making multiple queries but makes sure to have the most number of possible Facebook Pages/Users per company. 

The first correction is to add the cleaned legal name. The  module cleans the name from company's legal form so as to make a reasonable request to GraphAPI that returns a bunch of pages that match with the given query.

An example of a cleaned name is: 


\topskip0pt
\vspace*{\fill}
$ "Spaziodati\ S.R.L" \Rightarrow "Spaziodati" $
\vspace*{\fill}



A second cleaned name to add is the one with patterns associated to company legal forms removed:


\topskip0pt
\vspace*{\fill}
$ "Bar\ Roma\ di\ Rossi\ Carlo\ e\ figli" \Rightarrow "Bar\ Roma"$
\vspace*{\fill}



Other names added are the ones with all possible legal forms denomination alternatives. Atoka index in fact contains companies' name with their own legal form sign. The project used a dedicated function of Dandelion in order to generate all the options.

An example of one a new alternative is:

$ "ENI\ S.P.A" \Rightarrow "ENI\ Societa\ per\ Azioni" $

or can simply be:

$ "Enel\ S.P.A" \Rightarrow "Enel\ SPA" $

Another step is to change the legal name with the many possible alternatives names contained in Atoka index.

A plausible example is:

$ "CervedGroup\ S.P.A" \Rightarrow "CG\ SPA" $

Even combinations of the alternatives are made:

$ "CervedGroup\ S.P.A" \Rightarrow "CG\ Societa\ per\ Azioni" $

An important thing made achievable by Atoka data is to add to the list signs' names. Signs' names are names contained in the signs outside the companies and in many cases represent the real name of the Facebook Page/User. This fact happen because it's not unusual to have different local stores with different names that belong to a single holding company that will have nothing to do with the single marketing strategies and social activities.

That cases are usual initials, in the form of:

$ "ABC\ S.N.C" \Rightarrow "Ristorante\ da\ Luigi" $

As briefly explained before, every name is added to a list and every company will have a query associated at each name in that list. This makes sure that every possible page name is searched inside Facebook Graph and stored in the new elasticsearch index. We found that is far better to call 10 queries for each company and find a reasonable amout of pages that may match rather than populate the index in less time but having less possibility to find a matching page.

For using GraphAPI, Facebook requires a Developer account, verified with an individual mobile telephone number. Each account can have different applications running at the same time. The applications are identified with a text token and each of them can make at most 100 million API calls per day\cite{apipolices}. Even if the documentation shows that limit, experimentation found that the real limit is around 600 calls per 600 seconds, per token \& per IP\cite{realapilimit}. This bottleneck is overtaken creating 5 different accounts with 5 applications each, with a total of 25 applications.
The module is structured in multiprocessing with one different process for each token, running simultaneously on a private server. A second trick used not to be banned from Facebook is making requests with a customized version of facepy\cite{facepy} library that changes proxy continuously and simulates request all around the world. Every request is made after a random delay timeout, chosen to simulate real world's typical application calls.


A batch request makes possible to query the API 50 times at one, optimizing the latency created by http's connection opening and closing.  Each query is composed by two different groups of graph Api subqueries, one group for user profiles and one group for Facebook pages. All the two groups of requests put together the names generated in the previous step. Each request return at most 30 entities, the limit is chosen not to have a too large amount of useless data: most of the time the right company page/user comes in the first 20-30 results.

\begin{figure}
\begin{tabular}{| l | l |}
    \hline
    Type & Attributes \\ \hline
    Page & id, name, link, about, email, location, likes\\
         & members, description, category, contact\_address,\\
         & general\_info, general\_manager, mission, phone,\\
         & website, cover\\
    \hline
    User & id, name, link, about, email, bio, work, location,\\
         & description, address, hometown, website, cover\\
    \hline
\end{tabular}
    \caption{List of attributes exposed by Facebook for Pages and Users through Graph API.}
    \label{Fig.3}
\end{figure}

\section{Cleaning and Normalizing Data}

Before saving all the pages and user results in the new elasticsearch index, the data is cleaned and normalized. This step is obligatory because pages and users have different fields and because very frequently data exists but can't be found in the right place. This happens because people seldom set up their company page/user profile without taking care of adding the right thing in the right place. For example, in a lot of different pages, the field \textit{"phone"} or \textit{"email"} is empty but you can find one or both of them in the \textit{"about"} field, that was originally thought to contain a brief description of the page and what it is about.

To make sure to have all the data cleaned and normalized the module combines the fields in a single structure. After having collected everything in one place, the module checks with a proper regex if possible domains and emails are contained in the structure and annotates the structure with ontologies extracted with Dandelion. Dandelion extract persons, telephone numbers, places, addresses and entities (like "data mining", "agriculture", "automobiles").
Telephone numbers and addresses are automatically normalized by Dandelion and can be directly checked with the same attributes contained in the Atoka index.

The only field that is not included in the structure is the location information, that is analyzed on its own to avoid problem of ambiguity finding, for example, people inside the structure.

$ \text{Ex:}\ "[..]\ Via\ Marco\ Polo,\ 13\ [...]\ \Rightarrow \{\} $

After having cleaned and normalized the information, data is stored in the new elasticsearch index. The primary key chosen to identify the tuples is the real Facebook ID: this choice becomes handy because it make sure that one page/user is store only once and that the index is redundancy free and consistent.

\section{Generating Similarity Scores}
After the data is normalized and stored we need to generate the similarity score between the company and the possible Facebook pages. This is a list of how the scores are gives for each property:

\begin{itemize}
    \item WEBSITE
	Check if the sites are present: 0.5 pts if not.
	Matching of the normalized sites: 1pt if positive, 0 if negative.
	Sites normalization works removing from the string all the unnecessary parts, such as "hhtp://", "https://", "www.", we then match the two strings fully
	
    \item EMAIL
	The results are in form of tuple containing the resulting scores.
	First we check the presence of the email addresses: (0.5, 0.5) pts if not present.
	Next we match the various components of the input email with the company registry. The email is split in it's three main components: username, domain and ext, and for the companies we use data such as complete name, city and province code.
	For every input we produce the rankings with the company registry, using username first and domain in another run.
	Each run we clean up the input data, we fetch the province's full name and create a token list that are in common with the email and the processed data.
	Next pass is to check if every input token exceedes a similarity threshold, set at 75\% of the total length. Then the suitable ones are divided into two categories, common matches and uncommon matches.
	The token total score is calculated, adding up all the 'common matches'. Same for the 'uncommon matches'.
	Finally we produce the final score, choosing the highest couple
	
	
	
	$ \left(\frac{scoreCommon}{length(username)},\frac{scoreUncommon}{length(username)}\right)$
	
	
	
	%chiedo a lorenzo come mai si normalizzi con len (host)

    The same thing is done for domain:
    
    
    $ \left(\frac{scoreCommon}{length(domain)},\frac{scoreUncommon}{length(domain)}\right)$
   
   
    \item PHONE
	Check if the phone numbers are present: 0.5 pts if not.
	Matching of the inputs filtered from the empty ones: 1 pt if positive, 0 if negative.
	

    \item NAME
	Check for the presence of a description or a name in the facebook input: 0.5 pt if not present.
	We firstly checked in the description if there was at least one of the alternative names of the company: 1 pt if at least a match is present, 0 otherwise.
	We decided to optimize the compare in order to avoid to give a too high rate for too usual name. In fact it happens frequently to have situation where different companies have a very similar name:
	
	$ Pizza\ Vesuvio\ di\ Nino\ Coppola \simeq Pizza\ Vesuvio $
	
	the formula chosen to overtake this problem is:
	
	$
		\frac{\#tokenAtoka \cap \#tokenFacebook}{\#tokenAtoka \cup \#tokenFacebook}
	$

    \item ADDRESS
	For this section we calculate multiple scores for each part that the Facebook input address is consisting of.
	\begin{itemize}
    
        \item STREET
		Check for the street presence: 0.5 pts if not.
		Matching using the subdivision of the input streets in tokens, and then analyzing their set intersection using the formula:
		
		$
		\frac{\#tokenAtoka \cap \#tokenFacebook}{\#tokenAtoka \cup \#tokenFacebook}
	    $
	    
	\item HOUSE NUMBER
		Exact match between the input numbers: 1 pt if positive, 0 pts if negative
		or
		The function can also calculate a score determined by the number distance of the 2 inputs, using the formula:
		
		$
		1.5^{-|house\_number\_1-house\_number\_2|}
		$
	\item CITY
		Check for the city presence: 0.5 pts if not
		Exact match between the two cities: 1 pt if positive, 0 if negative.
		
	\item PROVINCE
		Check the presence of the province: 0.5 pts if not
		Exact match between the provinces 1 pt if positive, 0 otherwise
		
	\item ZIP CODE
		Check the presence of the zip code: 0.5 pts if not.
		Exact match between the two zip codes: 1 pt if positive, 0 otherwise
	
	\item TOTAL
		Results are in form of tuples having 5 scores each. Firstly we control the presence of all the above scores, and put a 0.5 if they are absent (or 0 in case of the house number)
		Each tuple is of the form:
		
		$
		(ScoreStreet,\ ScoreHouseNumber,\ ScoreCity, \\ ScoreProvince,\ ScoreZipCode)
		$
		
		If no score is present, the resulting tuple is:
		
		$(0.5,\ 0,\ 0.5,\ 0.5,\ 0.5)$
		
		Otherwise, we choose the best scoring tuple, from all the comparison of all the Facebook addresses with the Atoka input.
		
	\end{itemize}
	
    \item ENTITIES
	Score given by the intersection of the input data entities using the following formula:
	
	$
	\frac{EntitiesAtoka \cap EntitiesFacebook}{EntitiesAtoka}
	$
\end{itemize}

\section{Training Random Forest Classifier}

After the process of score assignment a subset of results is used to to train the classifier. The algorithm chosen for the machine learning classification is random forest.

%TOLGO CHIACCHIERE RF, AGGIUNGO PARTE DEL RANDOMIZE

A random forest is a type of classification algorithm that uses a multitude of decision trees for finding the resulting class of an input entity. The output is defined by the mode of the classes of all the decision trees that operate each one on a subset of the total of the attributes given to the classifier. The main principle is that using a lot of the so called "weak learners" (the single decision tree) on all the same target, we can obtain a "strong learner" (the random forest) made by their single valuations. Basing the conclusion on the mode of the single outputs has a drawback, as more and more trees are used the error rate will rise but contrariwise, as we lower them, the strength of the single element goes down: a difficult task is finding the number m of elements.

Random Forest is generally faster than the other types of classification algorithms, and this is one of the reasons because it's the best algorithm found for this problem\cite{randomforestart}. In a slightly different problem, the classification of websites instead of Facebook pages/users, SVM was implemented but the running time was tens of times longer than Random Forest and the results were even worse. This shows the second reason of why Random Forest seems to be better than SVM or other approaches: it's easier and faster to set, to regulate and to train.
Other important facts that showed Random Forest Classification Algorithm as the best for this job are that it's very good avoiding overfitting, that it's solid with outliers and that it works fine with partial and dirty data.

\subsection*{The comparable Website Problem}
%inzio
When the project started the first thing that has been taken in consideration was a similar project previously developed at Spaziodati. The aim of that project was to match website crawled from Italian net with the same company index that was used for this project.

The first step in order to get the official website classification was building a crawler for Italian websites network that could populate an index. That bot crawled all the websites looking for the same informations found in the company index (descriptions, emails, entities, location, social media and obviously the website URL).

After having created and populated the index, information were cleaned and normalized with Dandelion, in a really similar way of what was done in the Facebook project. Once the data was made comparable, as done here, a proprietary algorithm generated a similarity score for each of the websites' fields.

One time the scores were given, the team that was working on the project faced our same problem of choosing the best algorithm for the classification. Unfortunately, instead of having a similar project as we had, they had nothing to compare the project with and to choose the algorithm on the high level so they tried different solutions and took the best one.

Many algorithms were tried but the main two were:
\begin{itemize}
\item Random Forest
\item Support Vector Machine (SVM).
\end{itemize}

After the tests, the team decided to choose Random Forest mainly for performance of precision and recall over that dataset (obviously this information can't be generalized all over the data available in the world).

In that case Random Forest was the more convenient because it returned values of features that were really useful in order to evaluate the data gathered and to decide how deep the crawler needed to look in the process of data extraction fro the web.
A second important fact that drove the team in the algorithm decision was that the training time was far less than any other method, a fundamental consideration taking care of the fact that the index need to be continuously updated. Furthermore, Random Forest gives the developer the possibility to work with categorical features without too many manipulations. 
Last but not least, Random Forest in the best algorithm that an analyst can choose in order to avoid the overfitting problem.

As revealed before, when we had to choose the best algorithm to classify Facebook pages data, instead of taking care of testing the different algorithms - a process that really makes a company waste a lot of time and money -  we decided to compare the data available in the facebook index with the data available in the website's. The result was that the informations were really similar - as said before the two indexes were cleaned and normalized with really alike methods - so we directly decided to use Random Forest for our classification. Before taking the final decision we also decided to try SVM too because we wanted to be 100\% sure about the fact that it was the best also in our case but in less that a week we faced that, apart from the fact that was really harder to configure it right, it was really too slow for our needs and we firmly opted for Random Forest.

%fine

\subsection*{Random Forest in the Facebook Problem}

The first step when somebody is working with a machine learning algorithm is finding a useful training subset of the data. The main training subset is composed by 60\% of the data, splitted randomly from the main dataset.
Using the Random Forest Algorithm it is not necessary to define a custom cross validation system because it already have its own method to estimate the out-of-bag error. Even if the algorithm comes with this "simplification" we chose not to use it and to implement a cross validation. This choice was driven by the fact that the dataset is really skewed, with only few positive examples and few association between companies contained in Atoka index and theirs Facebook pages. The approach used is stratified three fold cross validation: the training set is sampled in the three fold, making sure to maintain the same positive/negative percentage composition, and for three time the random forest is trained with one fold and tested on the other two.
At the end, the trained classifier is tested with the remaining 40\% of the data.

In the training step, instead of giving parameters as possible values, the approach used was to give them as distribution with the Randomize Grid Search method. Randomize Grid Search randomly samples the distribution, then tests the different parameters and takes the better ones. The method uses 100 samples for the training dataset. This method works fine with this problem because it gives decent results if the distribution is uniform (as it is) and works in a reasonable amount of time.

The results with the first training set were not very good. This problem was caused by the fact that ATECO n.56's companies only had 6,000 pages/users classified and because many of that pages were false positive and many other pages that were missing in the initial index were found right in the training set.
To avoid this huge problem we opened a story on CrowdFlower
%\footnote{\url{http://www.crowdflower.com/}}
, asking people to verify if the pages associated by the classifier were right, wrong. This process help us redesigning the training set without false positive and true negative. This may seems a conscious modification of the training dataset to have "better final result" but, in reality, it is not overfitting because it only fix error that should not be in the first dataset. This solution was the only adoptable because the only platform that surely knows if a page is really about a company is Facebook itself.

